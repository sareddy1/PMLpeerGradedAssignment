---
title: "Practical Machine Learning Peer Graded Assignment - Course Project"
author: "Sudhakar A."
date: "March 19, 2017"
output:
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(caret) 
library(rattle) 
library(rpart) 
library(rpart.plot)
library(randomForest) 
library(repmis)
```

## Introduction
In this project, the given data includes activity data from accelerometers on the belt, forearm, arm, and dumbell of 6 research study participants. The training data consists of accelerometer data and a label identifying the quality of the activity the participant was doing. The test data consists of accelerometer data without the identifying label. Our goal is to predict the labels for the test set observations.

The following sections show the code used for and description on getting and processing the data, model estimation using training datasets and model application on validation data sets. 

## Data Processing
### Getting Data
Read the training and test data sets.

```{r}
trainUrl <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
testUrl <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"

training <- read.csv(url(trainUrl), na.strings=c("NA","#DIV/0!",""))
testing <- read.csv(url(testUrl), na.strings=c("NA","#DIV/0!",""))
dim(training); dim(testing)
```

The training dataset has 19,622 observations and 160 variables, and the testing data set contains 20 observations and the same variables as the training set. We are trying to predict the outcome of the variable classe in the training set.

### Cleaning data
We now delete columns (predictors) of the training set that contain any missing values.

```{r}
training <- training[, colSums(is.na(training)) == 0]
testing <- testing[, colSums(is.na(testing)) == 0]
```
We also remove the first seven predictors since these variables have little predicting power for the outcome classe.

```{r}
trainData <- training[, -c(1:7)]
testData <- testing[, -c(1:7)]
dim(trainData); dim(testData)
#names(trainData)
#names(testData)
```

The cleaned data sets trainData and testData both have 53 columns with the same first 52 variables and the last variable classe and  problem_id individually. trainData has 19,622 rows while testData has 20 rows.

### Data spliting
In order to get out-of-sample errors, we split the cleaned training set trainData into a training set (train, 70%) for prediction and a validation set (valid 30%) to compute the out-of-sample errors.

```{r}
set.seed(7826) 
inTrain <- createDataPartition(trainData$classe, p = 0.7, list = FALSE)
train <- trainData[inTrain, ]
valid <- trainData[-inTrain, ]
```

## Prediction Algorithms
We use classification trees and random forests to predict the outcome.

### Classification trees
In practice, k = 5 or k = 10 when doing k-fold cross validation. Here we consider 5-fold cross validation (default setting in trainControl function is 10) when implementing the algorithm to save a little computing time. Since data transformations may be less important in non-linear models like classification trees, we do not transform any variables.

```{r}
control <- trainControl(method = "cv", number = 5)
fit_rpart <- train(classe ~ ., data = train, method = "rpart", 
                   trControl = control)
print(fit_rpart, digits = 4)
fancyRpartPlot(fit_rpart$finalModel)

# predict outcomes using validation set
predict_rpart <- predict(fit_rpart, valid)
# Show prediction result
(conf_rpart <- confusionMatrix(valid$classe, predict_rpart))
(accuracy_rpart <- conf_rpart$overall[1])
```

From the confusion matrix, the accuracy rate is 0.5, and so the out-of-sample error rate is 0.5. Using classification tree does not predict the outcome 'classe' very well.

### Random forests
Since classification tree method does not perform well, we try random forest method instead.

```{r}
fit_rf <- train(classe ~ ., data = train, method = "rf", 
                   trControl = control)
print(fit_rf, digits = 4)

# predict outcomes using validation set
predict_rf <- predict(fit_rf, valid)
# Show prediction result
(conf_rf <- confusionMatrix(valid$classe, predict_rf))
(accuracy_rf <- conf_rf$overall[1])
```

For this dataset, random forest method is way better than classification tree method. The accuracy rate is 0.994, and so the out-of-sample error rate is 0.006. This may be due to the fact that many predictors are highly correlated. Random forests chooses a subset of predictors at each split and decorrelate the trees. This leads to high accuracy, although this algorithm is sometimes difficult to interpret and computationally inefficient.

##Prediction on Test Set
We now use random forests to predict the outcome variable classe for the testing set.

```{r}
(predict(fit_rf, testData))
```
